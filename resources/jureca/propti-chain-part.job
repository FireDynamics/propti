#!/bin/bash

# Provide job name to be displayed in the queue overview
#SBATCH --job-name propti-test

# Set desired number of nodes.
# JURECA provides 24 cores per node.
# The first 24 jobs per node get one physical core each. If more
# jobs are scheduled per node, hyperthreading is invoked.
#SBATCH --nodes 2
#SBATCH --ntasks 18

# Set number of cores that are to be used per task. For example when
# multiple cores are to be used per mesh, to speed up the calculation.
#SBATCH --cpus-per-task 3
#SBATCH --output %j.out
#SBATCH --error %j.err

# Wall clock time is 24 h on JURECA
# Set time constrains for the job execution, by the following format:
# days-hours:minutes:seconds   -    example 5-12:35:42
#SBATCH --time=05:30:00

# This signal is used to control the relaunch process of the simulation.
# It is needed when e.g. large FDS simulations are conducted, that would
# have longer execution times then the 24 h possible.
# Not necessary for propti.
## SBATCH --signal=B:SIGUSR1@600


cd $SLURM_SUBMIT_DIR


# load necessary modules
ml use /usr/local/software/jureca/OtherStages
ml Stages/2018b

module use -a ~arnold1/modules_fire/
ml spotpy/1.5.13-py3.6.6_IntelCompiler_2019.0_ParaStationMPI_5.2.1


pwd

printenv &> user.env-$SLURM_JOB_ID

which python3

# Add an extra line to the propti_db, to be able to destinguish between restarts (e.g. job crash).
# These markers are also looked for by the '--clean_db' function of the post processing scripts.
if [ -f propti_db.csv ]; then
   echo "#Restart#" >> propti_db.csv
fi

# Run PROPTI. Also, create a log file to save the output generated.
# Assumption that PROPTI is stoed two layers above this script.
# CHECK PATH FOR YOUR CASE!

srun --export=ALL python3.6 ../propti/propti_run.py . &>>log.spotpy_mpi
